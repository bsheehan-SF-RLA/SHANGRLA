{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONEAudit Demo\n",
    "\n",
    "ONEAudit (Refereed paper: https://link.springer.com/chapter/10.1007/978-3-031-48806-1_5 Full version: https://arxiv.org/abs/2303.03335 ) is a way to use batch tally information efficiently in RLAs. \n",
    "It is vastly more efficient than batch-level comparison auditing.\n",
    "For each SHANGRLA assertion, ONEAudit creates an \"average\" assorter value for each reporting batch.\n",
    "That is then used in a ballot-level comparison audit based on comparing the assorter applied to the manually ascertained vote (MVR, manual vote record) on each ballot card the audit selects to the average assorter value for the batch to which each ballot belongs.\n",
    "\n",
    "ONEAudit is useful in a variety of situations, including:\n",
    "\n",
    "+ when batch tallies are available, whether or not the batches correspond to physical batches, in contrast to traditional batch-level comparison audits, which work only when reporting batches correspond to physical batches\n",
    "+ when CVRs are available for batches of cards but there is no mapping from individual cards to individual CVRs (this is common for precinct-count optical scan systems)\n",
    "+ when CVRs are available for some cards but not others (ONEAudit improves on SUITE in that situation, avoiding the need for stratification)\n",
    "\n",
    "`SHANGRLA` currently infers batch information from Dominion CVRs as currently (that is, as of August 2024) used by San Francisco, as follows:\n",
    "\n",
    "+ when reading the CVRs using `shangrla.formats.Dominion.read_cvrs()`, set `pool_groups` to be the list of `CountingGroupID`s that should be audited using ONEAudit CVRs derived from batch tallies.\n",
    "    - For SF, that is `CountingGroupID == 1`.\n",
    "    - if `pool_groups` is nonempty, the CVRs with `CountingGroupID in pool_groups` are marked as `pool`\n",
    "+ apply `shangrla.core.Audit.CVR.check_tally_pools()`: ensure every CVR in each `tally_pool` has the same value of `pool`\n",
    "+ apply`shangrla.core.Audit.CVR.add_pool_contests()`: ensure every CVR in each `tally_pool` for which `pool == True` has every contest in the tally_pool\n",
    "+ create the assertions for the contests under audit using functions in `shangrla.core.Audit.Assertion`\n",
    "+ apply `shangrla.core.Audit.Assorter.set_tally_pool_means()` to set the assorter means\n",
    "+ estimate initial sample size\n",
    "+ audit then proceeds as a standard ballot-level comparison audit. The MVR for each inspected card will automatically be compared to the ONEAudit CVRs for the batch to which that card belongs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow for a ONEAudit RLA\n",
    "\n",
    "+ Read overall audit information (including the seed) and contest information\n",
    "+ Read RAIRE assertions for IRV contests and construct assertions for all other contests\n",
    "+ Read ballot manifest\n",
    "+ For comparison audits, read cvrs. Every CVR should have a corresponding manifest entry.\n",
    "+ Prepare ~2EZ:\n",
    "    - `N_phantoms = max_cards - cards_in_manifest`\n",
    "    - If `N_phantoms < 0`: complain\n",
    "    - Else: create `N_phantoms` phantom cards\n",
    "    - For each contest `c`:\n",
    "        + `N_c` is the input upper bound on the number of cards that contain `c`\n",
    "        + If `N_c is None`: `N_c = max_cards - non_c_cvrs`, where `non_c_cvrs` is #CVRs that don't contain `c`\n",
    "        + `C_c` is the number of CVRs that contain the contest\n",
    "        + If `C_c > N_c`: complain\n",
    "        + Else if `N_c - C_c > N_phantoms`: complain\n",
    "        + Else:\n",
    "            - Consider contest `c` to be on the first `N_c - C_c` phantom CVRs\n",
    "            - Consider contest `c` to be on the first `N_c - C_c` phantom ballots\n",
    "+ Create Assertions for every Contest. This involves also creating an Assorter for every Assertion, and a `NonnegMean` test\n",
    "for every Assertion.\n",
    "+ Calculate assorter margins for all assorters:\n",
    "    - If `not use_style`: apply the Assorter to all cards and CVRs, including phantoms\n",
    "    - Else: apply the assorter only to cards/cvrs reported to contain the contest, including phantoms that contain the contest\n",
    "+ Set `assertion.test.u` to the appropriate value for each assertion: `assorter.upper_bound` for polling audits or \n",
    "      `2/(2-assorter.margin/assorter.upper_bound)` for ballot-level comparison audits\n",
    "+ Estimate starting sample size for the specified sampling design (w/ or w/o replacement, stratified, etc.), for chosen risk function, use of card-style information, etc.:\n",
    "    - User-specified criterion, controlled by parameters. Examples:\n",
    "        + expected sample size for completion, on the assumption that there are no errors\n",
    "        + 90th percentile of sample size for completion, on the assumption that errors are not more frequent than specified\n",
    "    - If `not use_style`: base estimate on sampling from the entire manifest, i.e., smallest assorter margin\n",
    "    - Else: use consistent sampling:\n",
    "        + Augment each CVR (including phantoms) with a probability of selection, `p`, initially 0\n",
    "        + For each contest `c`:\n",
    "            - Find sample size `n_c` that meets the criterion \n",
    "            - For each non-phantom CVR that contains the contest, set `p = max(p, n_c/N_c)` \n",
    "        + Estimated sample size is the sum of `p` over all non-phantom CVRs\n",
    "+ Draw the random sample:\n",
    "    - Use the specified design, including using consistent sampling for style information\n",
    "    - Express sample cards in terms of the manifest\n",
    "    - Export\n",
    "+ Read manual interpretations of the cards (MVRs)\n",
    "+ Calculate attained risk for each assorter\n",
    "    - Use ~2EZ to deal with phantom CVRs or cards; the treatment depends on whether `use_style == True`\n",
    "+ Report\n",
    "+ Estimate incremental sample size if any assorter nulls have not been rejected\n",
    "+ Draw incremental sample; etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audit parameters.\n",
    "\n",
    "The overall audit involves information that is the same across contests, encapsulated in\n",
    "a dict called `audit`:\n",
    "\n",
    "* `seed`: the numeric seed for the pseudo-random number generator used to draw sample (for SHA256 PRNG)\n",
    "* `sim_seed`: seed for simulations to estimate sample sizes (for Mersenne Twister PRNG)\n",
    "* `quantile`: quantile of the sample size to use for setting initial sample size\n",
    "* `cvr_file`: filename for CVRs (input)\n",
    "* `manifest_file`: filename for ballot manifest (input)\n",
    "* `use_style`: Boolean. If True, use card style information (inferred from CVRs) to target samples. If False, sample from all cards, regardless of the contest.\n",
    "* `sample_file`: filename for sampled card identifiers (output)\n",
    "* `mvr_file`: filename for manually ascertained votes from sampled cards (input)\n",
    "* `log_file`: filename for audit log (output)\n",
    "* `error_rate_1`: expected rate of 1-vote overstatements. Recommended value $\\ge$ 0.001 if there are hand-marked ballots. Larger values increase the initial sample size, but make it more likely that the audit will conclude after a single round even if the audit finds errors\n",
    "* `error_rate_2`: expected rate of 2-vote overstatements. 2-vote overstatements should be extremely rare.\n",
    "Recommended value: 0. Larger values increase the initial sample size, but make it more likely that the audit will conclude after a single round even if the audit finds errors\n",
    "* `reps`: number of replications to use to estimate sample sizes. If `reps is None`, uses a deterministic method\n",
    "* `quantile`: quantile of sample size to estimate. Not used if `reps is None`\n",
    "* `strata`: a dict describing the strata. Keys are stratum identifiers; values are dicts containing:\n",
    "    + `max_cards`: an upper bound on the number of pieces of paper cast in the contest. This should be derived independently of the voting system. A ballot consists of one or more cards.\n",
    "    + `replacement`: whether to sample from this stratum with replacement. \n",
    "    + `use_style`: True if the sample in that stratum uses card-style information.\n",
    "    + `audit_type` one of Contest.POLLING, Contest.CARD_COMPARISON, Contest.BATCH_COMPARISON but only POLLING and CARD_COMPARISON are currently implemented. \n",
    "    + `test`: the name of the function to be used to measure risk. Options are `kaplan_markov`,`kaplan_wald`,`kaplan_kolmogorov`,`wald_sprt`,`kaplan_mart`, `alpha_mart`, `betting_mart`. \n",
    "Not all risk functions work with every social choice function or every sampling method. \n",
    "    + `estim`: the estimator to be used by the `alpha_mart` risk function. Options:  \n",
    "        - `fixed_alternative_mean` (default)\n",
    "        - `shrink_trunc`\n",
    "        - `optimal_comparison`\n",
    "    + `bet`: the method to select the bet for the `betting_mart` risk function. Options:\n",
    "        - `fixed_bet` (default)\n",
    "        - `agrapa`\n",
    "    + `test_kwargs`: keyword arguments for the risk function\n",
    "\n",
    "----\n",
    "\n",
    "* `contests`: a dict of contest-specific data \n",
    "    + the keys are unique contest identifiers for contests under audit\n",
    "    + the values are Contest objects with attributes:\n",
    "        - `risk_limit`: the risk limit for the audit of this contest\n",
    "        - `cards`: an upper bound on the number of cast cards that contain the contest\n",
    "        - `choice_function`: `Audit.SOCIAL_CHOICE_FUNCTION.PLURALITY`, \n",
    "          `Audit.SOCIAL_CHOICE_FUNCTION.SUPERMAJORITY`, or `Audit.SOCIAL_CHOICE_FUNCTION.IRV`\n",
    "        - `n_winners`: number of winners for majority contests. (Multi-winner IRV, aka STV, is not supported)\n",
    "        - `share_to_win`: for super-majority contests, the fraction of valid votes required to win, e.g., 2/3.\n",
    "           (share_to_win*n_winners must be less than 100%)\n",
    "        - `candidates`: list of names or identifiers of candidates\n",
    "        - `reported_winners` : list of identifier(s) of candidate(s) reported to have won.\n",
    "           Length should equal `n_winners`.\n",
    "        - `assertion_file`: filename for a set of json descriptors of Assertions (see technical documentation) that collectively imply the reported outcome of the contest is correct. Required for IRV; ignored for other social choice functions\n",
    "        - `audit_type`: the audit strategy. Currently `Audit.AUDIT_TYPE.POLLING (ballot-polling)`, \n",
    "           `Audit.AUDIT_TYPE.CARD_COMPARISON` (ballot-level comparison audits), and `Audit.AUDIT_TYPE.ONEAUDIT`\n",
    "            are implemented. HYBRID and STRATIFIED are planned.\n",
    "        - `test`: the risk function for the audit. Default is `NonnegMean.alpha_mart`, the alpha supermartingale test\n",
    "        - `estim`: estimator for the alternative hypothesis for the test. Default is `NonnegMean.shrink_trunc`\n",
    "        - `use_style`: True to use style information from CVRs to target the sample. False for polling audits or for sampling from all ballots for every contest.\n",
    "        - other keys and values are added by the software, including `cvrs`, the number of CVRs that contain the contest, and `p`, the sampling fraction expected to be required to confirm the contest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if shangrla has not already been installed, install it then restart the kernel\n",
    "# !pip install -e \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import copy\n",
    "\n",
    "import glob\n",
    "import os, sys\n",
    "\n",
    "from collections import OrderedDict\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from cryptorandom.cryptorandom import SHA256, int_from_hash\n",
    "from cryptorandom.sample import sample_by_index\n",
    "\n",
    "from shangrla.core.Audit import Audit, Assertion, Assorter, Contest, CVR, Stratum\n",
    "from shangrla.core.NonnegMean import NonnegMean\n",
    "from shangrla.formats.Dominion import Dominion\n",
    "\n",
    "sys.path.append(os.path.realpath('./SHANGRLA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "audit = Audit.from_dict({\n",
    "         'seed':           12345678901234567890,\n",
    "         'sim_seed':       314159265,\n",
    "         'cvr_file':       './data/SF_CVR_Export_20240311150227/CvrExport_*.json',\n",
    "         'manifest_file':  './data/SF_CVR_Export_20240311150227/dummy.xlsx',\n",
    "         'sample_file':    './data/sample.csv',\n",
    "         'mvr_file':       './data/mvr.json',\n",
    "         'log_file':       './data/log.json',\n",
    "         'quantile':       0.8,\n",
    "         'error_rate_1':   0.001,\n",
    "         'error_rate_2':   0.0,\n",
    "         'reps':           100,\n",
    "         'strata':         {'stratum_1': {'max_cards':   293555, \n",
    "                                          'use_style':   True,\n",
    "                                          'replacement': False,\n",
    "                                          'audit_type':  Audit.AUDIT_TYPE.CARD_COMPARISON,\n",
    "                                          'test':        NonnegMean.alpha_mart,\n",
    "                                          'estimator':   NonnegMean.optimal_comparison,\n",
    "                                          'test_kwargs': {}\n",
    "                                         }\n",
    "                           }\n",
    "        })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmax_cards = 293555 # 146662 VBM turnout per SF Elections release 12 \\nhttps://sfelections.sfgov.org/november-5-2019-election-results-summary\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find upper bound on total cards across strata\n",
    "audit.max_cards = np.sum([s.max_cards for s in audit.strata.values()])\n",
    "'''\n",
    "max_cards = 293555 # 146662 VBM turnout per SF Elections release 12 \n",
    "https://sfelections.sfgov.org/november-5-2019-election-results-summary\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "# extract contests and candidates from relevant manifests\n",
    "# ingest a list of contests to audit; include max cards cast in each contest or a flag to use the CVRs as an approximation\n",
    "# find winner(s) of the audited contests from CVRs\n",
    "# set \"boilerplate\" variables: risk limits, cards, audit_type, etc.\n",
    "# construct contest dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contests to audit. Edit with details of your contest (eg., Contest 339 is the DA race)\n",
    "contest_dict = {'1':{\n",
    "                   'name': 'PRESIDENT OF THE UNITED STATES-DEM',\n",
    "                   'risk_limit':       0.05,\n",
    "                   'cards':            443578,\n",
    "                   'choice_function':  Contest.SOCIAL_CHOICE_FUNCTION.PLURALITY,\n",
    "                   'n_winners':        1,\n",
    "                   'candidates':       ['5',],\n",
    "                   'winner':           ['5'],\n",
    "                   'assertion_file':   None,\n",
    "                   'audit_type':       Audit.AUDIT_TYPE.CARD_COMPARISON,\n",
    "                   'test':             NonnegMean.alpha_mart,\n",
    "                   'estim':            NonnegMean.optimal_comparison\n",
    "                  },\n",
    "               '2':{\n",
    "                   'name': 'PRESIDENT OF THE UNITED STATES-REP',\n",
    "                   'risk_limit':       0.05,\n",
    "                   'cards':            443578,\n",
    "                   'choice_function':  Contest.SOCIAL_CHOICE_FUNCTION.PLURALITY,\n",
    "                   'n_winners':        1,\n",
    "                   'candidates':       ['9','10','11','12'],\n",
    "                   'winner':           ['11'],\n",
    "                   'assertion_file':   None,\n",
    "                   'audit_type':       Audit.AUDIT_TYPE.CARD_COMPARISON,\n",
    "                   'test':             NonnegMean.alpha_mart,\n",
    "                   'estim':            NonnegMean.optimal_comparison\n",
    "                  }\n",
    "               }\n",
    "\n",
    "contests = Contest.from_dict_of_dicts(contest_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of other social choice functions:\n",
    "\n",
    "        contests =  {'city_council':{'name': 'City Council',\n",
    "                             'risk_limit':0.05,\n",
    "                             'cards': None,\n",
    "                             'choice_function': Contest.SOCIAL_CHOICE_FUNCTION.PLURALITY,\n",
    "                             'n_winners':3,\n",
    "                             'candidates':['Doug','Emily','Frank','Gail','Harry'],\n",
    "                             'winner' : ['Doug', 'Emily', 'Frank']\n",
    "                            },\n",
    "                        'measure_1':{'name': 'Measure 1',\n",
    "                             'risk_limit':0.05,\n",
    "                             'cards': 65432,\n",
    "                             'choice_function': Contest.SOCIAL_CHOICE_FUNCTION.SUPERMAJORITY,\n",
    "                             'share_to_win':2/3,\n",
    "                             'n_winners':1,\n",
    "                             'candidates':['yes','no'],\n",
    "                             'winner' : ['yes']\n",
    "                            }                  \n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the assertions for the IRV contest\n",
    "for c in contests:\n",
    "    if contests[c].choice_function == Contest.SOCIAL_CHOICE_FUNCTION.IRV:\n",
    "        with open(contests[c].assertion_file, 'r') as f:\n",
    "            contests[c].assertion_json = json.load(f)['audits'][0]['assertions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct the dict of dicts of assertions for each contest\n",
    "Assertion.make_all_assertions(contests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "audit.check_audit_parameters(contests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the ballot manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/SF_CVR_Export_20240311150227/dummy.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# special for Primary/Dominion manifest format\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m manifest \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(audit\u001b[38;5;241m.\u001b[39mmanifest_file)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/excel/_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[1;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m ExcelFile(\n\u001b[1;32m    496\u001b[0m         io,\n\u001b[1;32m    497\u001b[0m         storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    498\u001b[0m         engine\u001b[38;5;241m=\u001b[39mengine,\n\u001b[1;32m    499\u001b[0m         engine_kwargs\u001b[38;5;241m=\u001b[39mengine_kwargs,\n\u001b[1;32m    500\u001b[0m     )\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/excel/_base.py:1550\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m inspect_excel_format(\n\u001b[1;32m   1551\u001b[0m         content_or_path\u001b[38;5;241m=\u001b[39mpath_or_buffer, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[1;32m   1552\u001b[0m     )\n\u001b[1;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1557\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/excel/_base.py:1402\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1400\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[0;32m-> 1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[1;32m   1403\u001b[0m     content_or_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m, storage_options\u001b[38;5;241m=\u001b[39mstorage_options, is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1404\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[1;32m   1405\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   1406\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/SF_CVR_Export_20240311150227/dummy.xlsx'"
     ]
    }
   ],
   "source": [
    "# special for Primary/Dominion manifest format\n",
    "manifest = pd.read_excel(audit.manifest_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the CVR data and create CVR objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ballot-level comparison audits\n",
    "cvr_list = []\n",
    "for _fname in glob.glob(audit.cvr_file):\n",
    "    cvr_list.extend(Dominion.read_cvrs(_fname, use_current=True, enforce_rules=True, include_groups=(1,2)))\n",
    "    \n",
    "cvr_list = Dominion.raire_to_dominion(cvr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "443578"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cvr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'manifest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# double-check whether the manifest accounts for every card\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m audit\u001b[38;5;241m.\u001b[39mmax_cards, np\u001b[38;5;241m.\u001b[39msum(manifest[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal Ballots\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'manifest' is not defined"
     ]
    }
   ],
   "source": [
    "# double-check whether the manifest accounts for every card\n",
    "audit.max_cards, np.sum(manifest['Total Ballots'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tray #</th>\n",
       "      <th>Tabulator Number</th>\n",
       "      <th>Batch Number</th>\n",
       "      <th>Total Ballots</th>\n",
       "      <th>VBMCart.Cart number</th>\n",
       "      <th>cum_cards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>99808</td>\n",
       "      <td>78</td>\n",
       "      <td>116</td>\n",
       "      <td>3</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>99808</td>\n",
       "      <td>77</td>\n",
       "      <td>115</td>\n",
       "      <td>3</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>99808</td>\n",
       "      <td>79</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>99808</td>\n",
       "      <td>81</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>99808</td>\n",
       "      <td>80</td>\n",
       "      <td>116</td>\n",
       "      <td>3</td>\n",
       "      <td>543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5476</th>\n",
       "      <td>3506</td>\n",
       "      <td>99815</td>\n",
       "      <td>86</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>292557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5477</th>\n",
       "      <td>3506</td>\n",
       "      <td>99815</td>\n",
       "      <td>84</td>\n",
       "      <td>222</td>\n",
       "      <td>19</td>\n",
       "      <td>292779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5478</th>\n",
       "      <td>3506</td>\n",
       "      <td>99815</td>\n",
       "      <td>83</td>\n",
       "      <td>346</td>\n",
       "      <td>19</td>\n",
       "      <td>293125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5479</th>\n",
       "      <td>3506</td>\n",
       "      <td>99815</td>\n",
       "      <td>82</td>\n",
       "      <td>332</td>\n",
       "      <td>19</td>\n",
       "      <td>293457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5480</th>\n",
       "      <td>3507</td>\n",
       "      <td>99802</td>\n",
       "      <td>822</td>\n",
       "      <td>98</td>\n",
       "      <td>14</td>\n",
       "      <td>293555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5481 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Tray # Tabulator Number Batch Number  Total Ballots VBMCart.Cart number  \\\n",
       "0         1            99808           78            116                   3   \n",
       "1         1            99808           77            115                   3   \n",
       "2         1            99808           79            120                   3   \n",
       "3         1            99808           81             76                   3   \n",
       "4         1            99808           80            116                   3   \n",
       "...     ...              ...          ...            ...                 ...   \n",
       "5476   3506            99815           86              2                  19   \n",
       "5477   3506            99815           84            222                  19   \n",
       "5478   3506            99815           83            346                  19   \n",
       "5479   3506            99815           82            332                  19   \n",
       "5480   3507            99802          822             98                  14   \n",
       "\n",
       "      cum_cards  \n",
       "0           116  \n",
       "1           231  \n",
       "2           351  \n",
       "3           427  \n",
       "4           543  \n",
       "...         ...  \n",
       "5476     292557  \n",
       "5477     292779  \n",
       "5478     293125  \n",
       "5479     293457  \n",
       "5480     293555  \n",
       "\n",
       "[5481 rows x 6 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that there is a card in the manifest for every card (possibly) cast. If not, add phantoms.\n",
    "manifest, manifest_cards, phantom_cards = Dominion.prep_manifest(manifest, audit.max_cards, len(cvr_list))\n",
    "manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CVRs for phantom cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 0 phantom records\n"
     ]
    }
   ],
   "source": [
    "# For Comparison Audits Only\n",
    "#----------------------------\n",
    "\n",
    "# If the sample draws a phantom card, these CVRs will be used in the comparison.\n",
    "# phantom MVRs should be treated as zeros by the Assorter for every contest\n",
    "\n",
    "# setting use_style = False to generate phantoms\n",
    "\n",
    "cvr_list, phantom_vrs = CVR.make_phantoms(audit=audit, contests=contests, cvr_list=cvr_list, prefix='phantom-1-')\n",
    "print(f\"Created {phantom_vrs} phantom records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum assorter margin 0.019902906001554532\n",
      "margins in contest 339:\n",
      "\tassertion 18 v 17 elim 15 16 45: 0.045792366120740224\n",
      "\tassertion 17 v 16 elim 15 18 45: 0.019902906001554532\n",
      "\tassertion 15 v 18 elim 16 17 45: 0.028923647570604505\n",
      "\tassertion 18 v 16 elim 15 17 45: 0.0830003681935334\n",
      "\tassertion 17 v 16 elim 15 45: 0.058079120699294995\n",
      "\tassertion 15 v 17 elim 16 45: 0.08064120222007065\n",
      "\tassertion 15 v 17 elim 16 18 45: 0.10951712099930444\n",
      "\tassertion 18 v 16 elim 15 45: 0.14875018750596603\n",
      "\tassertion 15 v 16 elim 17 45: 0.13548158350492967\n",
      "\tassertion 15 v 16 elim 17 18 45: 0.1365247985163165\n",
      "\tassertion 15 v 16 elim 18 45: 0.16666893946625572\n",
      "\tassertion 15 v 16 elim 45: 0.15626406294745743\n",
      "\tassertion 15 v 45: 0.2956457705472446\n"
     ]
    }
   ],
   "source": [
    "# find the mean of the assorters for the CVRs and check whether the assertions are met\n",
    "min_margin = Assertion.set_all_margins_from_cvrs(audit=audit, contests=contests, cvr_list=cvr_list)\n",
    "\n",
    "print(f'minimum assorter margin {min_margin}')\n",
    "Contest.print_margins(contests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "audit.write_audit_parameters(contests=contests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up for sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find initial sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_size=372\n",
      "[('339', 372)]\n"
     ]
    }
   ],
   "source": [
    "# find initial sample size \n",
    "sample_size = audit.find_sample_size(contests, cvrs=cvr_list)  \n",
    "print(f'{sample_size=}\\n{[(i, c.sample_size) for i, c in contests.items()]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Draw the first sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# draw the initial sample using consistent sampling\n",
    "prng = SHA256(audit.seed)\n",
    "CVR.assign_sample_nums(cvr_list, prng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sample includes 0 phantom cards.\n"
     ]
    }
   ],
   "source": [
    "sampled_cvr_indices = CVR.consistent_sampling(cvr_list=cvr_list, contests=contests)\n",
    "n_sampled_phantoms = np.sum(sampled_cvr_indices > manifest_cards)\n",
    "print(f'The sample includes {n_sampled_phantoms} phantom cards.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146662, 293555, 293555)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cvr_list), manifest_cards, audit.max_cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for comparison audit\n",
    "cards_to_retrieve, sample_order, cvr_sample, mvr_phantoms_sample = \\\n",
    "    Dominion.sample_from_cvrs(cvr_list, manifest, sampled_cvr_indices)\n",
    "\n",
    "# for polling audit\n",
    "# cards_to_retrieve, sample_order, mvr_phantoms_sample = Dominion.sample_from_manifest(manifest, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the sample\n",
    "Dominion.write_cards_sampled(audit.sample_file, cards_to_retrieve, print_phantoms=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the audited sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for real data\n",
    "with open(audit.mvr_file) as f:\n",
    "    mvr_json = json.load(f)\n",
    "\n",
    "mvr_sample = CVR.from_dict(mvr_json['ballots'])\n",
    "\n",
    "# for simulated data, no errors\n",
    "mvr_sample = cvr_sample.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find measured risks for all assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "CVR.prep_comparison_sample(mvr_sample, cvr_sample, sample_order)  # for comparison audit\n",
    "# CVR.prep_polling_sample(mvr_sample, sample_order)  # for polling audit\n",
    "\n",
    "###### TEST\n",
    "# permute part of the sample to introduce errors deliberately\n",
    "mvr_sample = cvr_sample.copy()\n",
    "n_errs = 5\n",
    "errs = mvr_sample[0:n_errs].copy()\n",
    "np.random.seed(12345678)\n",
    "np.random.shuffle(errs)\n",
    "mvr_sample[0:n_errs] = errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum assertion p-value 0.48779308744628547\n",
      "\n",
      "p-values for assertions in contest 339\n",
      "\t18 v 17 elim 15 16 45: 0.47847909464463045\n",
      "\t17 v 16 elim 15 18 45: 0.48779308744628547\n",
      "\t15 v 18 elim 16 17 45: 0.44631352397209006\n",
      "\t18 v 16 elim 15 17 45: 2.877844184074244e-07\n",
      "\t17 v 16 elim 15 45: 0.0035354717267599813\n",
      "\t15 v 17 elim 16 45: 4.0458461637126434e-07\n",
      "\t15 v 17 elim 16 18 45: 2.3358046805505937e-07\n",
      "\t18 v 16 elim 15 45: 5.790407277111407e-13\n",
      "\t15 v 16 elim 17 45: 9.312863307985585e-12\n",
      "\t15 v 16 elim 17 18 45: 1.8495628594508403e-09\n",
      "\t15 v 16 elim 18 45: 1.1604860427653792e-14\n",
      "\t15 v 16 elim 45: 9.55405920633293e-14\n",
      "\t15 v 45: 1.373326785425354e-26\n",
      "\n",
      "contest 339 audit INCOMPLETE at risk limit 0.05. Attained risk 0.48779308744628547\n",
      "assertions remaining to be proved:\n",
      "\t18 v 17 elim 15 16 45\tcontest_id: 339 winner: 18 loser: 17 assorter: contest_id: 339\n",
      "upper bound: 1, winner defined: False, loser defined: False, assort defined: Truetally_pool_means: None p-value: 0.47847909464463045 margin: 0.045792366120740224 test: test: <bound method NonnegMean.alpha_mart of <shangrla.core.NonnegMean.NonnegMean object at 0x122b45df0>> estim: <bound method NonnegMean.optimal_comparison of <shangrla.core.NonnegMean.NonnegMean object at 0x122b45df0>> upper bound u: 1.0234327025065595 N: 146662 null mean t: 0.5 kwargs: {} p-history length: 372 proved: False sample_size: 130 assorter upper bound: 1 proved:  False sample_size: 130 : current risk 0.47847909464463045\n",
      "\t17 v 16 elim 15 18 45\tcontest_id: 339 winner: 17 loser: 16 assorter: contest_id: 339\n",
      "upper bound: 1, winner defined: False, loser defined: False, assort defined: Truetally_pool_means: None p-value: 0.48779308744628547 margin: 0.019902906001554532 test: test: <bound method NonnegMean.alpha_mart of <shangrla.core.NonnegMean.NonnegMean object at 0x177215280>> estim: <bound method NonnegMean.optimal_comparison of <shangrla.core.NonnegMean.NonnegMean object at 0x177215280>> upper bound u: 1.0100514798298927 N: 146662 null mean t: 0.5 kwargs: {} p-history length: 372 proved: False sample_size: 372 assorter upper bound: 1 proved:  False sample_size: 372 : current risk 0.48779308744628547\n",
      "\t15 v 18 elim 16 17 45\tcontest_id: 339 winner: 15 loser: 18 assorter: contest_id: 339\n",
      "upper bound: 1, winner defined: False, loser defined: False, assort defined: Truetally_pool_means: None p-value: 0.44631352397209006 margin: 0.028923647570604505 test: test: <bound method NonnegMean.alpha_mart of <shangrla.core.NonnegMean.NonnegMean object at 0x177b86780>> estim: <bound method NonnegMean.optimal_comparison of <shangrla.core.NonnegMean.NonnegMean object at 0x177b86780>> upper bound u: 1.0146740371244145 N: 146662 null mean t: 0.5 kwargs: {} p-history length: 372 proved: False sample_size: 207 assorter upper bound: 1 proved:  False sample_size: 207 : current risk 0.44631352397209006\n"
     ]
    }
   ],
   "source": [
    "p_max = Assertion.set_p_values(contests=contests, mvr_sample=mvr_sample, cvr_sample=cvr_sample)\n",
    "print(f'maximum assertion p-value {p_max}')\n",
    "done = audit.summarize_status(contests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the status of the audit \n",
    "audit.write_audit_parameters(contests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many more cards should be audited?\n",
    "\n",
    "Estimate how many more cards will need to be audited to confirm any remaining contests. The enlarged sample size is based on:\n",
    "\n",
    "* cards already sampled\n",
    "* the assumption that we will continue to see errors at the same rate observed in the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_size=7098\n",
      "[('339', 6726)]\n"
     ]
    }
   ],
   "source": [
    "# Estimate sample size required to confirm the outcome, if errors continue\n",
    "# at the same rate as already observed.\n",
    "\n",
    "new_size = audit.find_sample_size(contests, cvrs=cvr_list, mvr_sample=mvr_sample, cvr_sample=cvr_sample)\n",
    "print(f'{new_size=}\\n{[(i, c.sample_size) for i, c in contests.items()]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the first sample\n",
    "sampled_cvr_indices_old, cards_to_retrieve_old, sample_order_old, cvr_sample_old, mvr_phantoms_sample_old = \\\n",
    "    sampled_cvr_indices, cards_to_retrieve,     sample_order,     cvr_sample,     mvr_phantoms_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sample includes 0 phantom cards.\n"
     ]
    }
   ],
   "source": [
    "# draw the sample\n",
    "sampled_cvr_indices = CVR.consistent_sampling(cvr_list=cvr_list, contests=contests)\n",
    "n_sampled_phantoms = np.sum(sampled_cvr_indices > manifest_cards)\n",
    "print(f'The sample includes {n_sampled_phantoms} phantom cards.')\n",
    "\n",
    "# for comparison audit\n",
    "cards_to_retrieve, sample_order, cvr_sample, mvr_phantoms_sample = \\\n",
    "    Dominion.sample_from_cvrs(cvr_list, manifest, sampled_cvr_indices)\n",
    "\n",
    "# for polling audit\n",
    "# cards_to_retrieve, sample_order, mvr_phantoms_sample = Dominion.sample_from_manifest(manifest, sample)\n",
    "\n",
    "# write the sample\n",
    "# could write only the incremental sample using list(set(cards_to_retrieve) - set(cards_to_retrieve_old))\n",
    "Dominion.write_cards_sampled(audit.sample_file, cards_to_retrieve, print_phantoms=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for real data\n",
    "with open(audit.mvr_file) as f:\n",
    "    mvr_json = json.load(f)\n",
    "\n",
    "mvr_sample = CVR.from_dict(mvr_json['ballots'])\n",
    "\n",
    "# for simulated data, no errors\n",
    "mvr_sample = cvr_sample.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find measured risks for all assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "CVR.prep_comparison_sample(mvr_sample, cvr_sample, sample_order)  # for comparison audit\n",
    "# CVR.prep_polling_sample(mvr_sample, sample_order)  # for polling audit\n",
    "\n",
    "###### TEST\n",
    "# permute part of the sample to introduce errors deliberately\n",
    "mvr_sample = cvr_sample.copy()\n",
    "n_errs = 5\n",
    "errs = mvr_sample[0:n_errs].copy()\n",
    "np.random.seed(12345678)\n",
    "np.random.shuffle(errs)\n",
    "mvr_sample[0:n_errs] = errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum assertion p-value 1.2395432394747009e-28\n",
      "\n",
      "p-values for assertions in contest 339\n",
      "\t18 v 17 elim 15 16 45: 1.4449144036994632e-65\n",
      "\t17 v 16 elim 15 18 45: 1.2395432394747009e-28\n",
      "\t15 v 18 elim 16 17 45: 5.2774518365009846e-42\n",
      "\t18 v 16 elim 15 17 45: 6.205447790363212e-127\n",
      "\t17 v 16 elim 15 45: 2.8080650629218943e-86\n",
      "\t15 v 17 elim 16 45: 2.6490812247796027e-123\n",
      "\t15 v 17 elim 16 18 45: 1.7998086930305547e-166\n",
      "\t18 v 16 elim 15 45: 1.7022899132638306e-231\n",
      "\t15 v 16 elim 17 45: 4.74504262834718e-210\n",
      "\t15 v 16 elim 17 18 45: 2.445447527327654e-209\n",
      "\t15 v 16 elim 18 45: 9.191770721839916e-261\n",
      "\t15 v 16 elim 45: 8.342200191989171e-244\n",
      "\t15 v 45: 0.0\n",
      "\n",
      "contest 339 AUDIT COMPLETE at risk limit 0.05. Attained risk 1.2395432394747009e-28\n"
     ]
    }
   ],
   "source": [
    "p_max = Assertion.set_p_values(contests=contests, mvr_sample=mvr_sample, cvr_sample=cvr_sample)\n",
    "print(f'maximum assertion p-value {p_max}')\n",
    "done = audit.summarize_status(contests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "state": {
    "6cab9cab294247839758fa9e8d64d122": {
     "views": [
      {
       "cell_index": 42
      }
     ]
    },
    "b7b0321f834d45ebb1bdc036fba7a916": {
     "views": [
      {
       "cell_index": 38
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
